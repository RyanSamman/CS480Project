{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Questions\n",
    "\n",
    "- Strip outliers? Aux only, or Aux + labels\n",
    "- Types of regularization RobustScaler vs MinMaxScaler\n",
    "- Augment data \n",
    "- Aux Data\n",
    "- Image transformations ( Flip, rotate, blur, colorshift)\n",
    "- Freezing Layers vs not\n",
    "- resnet weights\n",
    "- Which pretrained model to use (Inception-Resnet-V2, Xception, MobileNetV2)\n",
    "- 6 models vs 1 model\n",
    "- Aux 6 vs 163 aux data\n",
    "- Multiple Heads\n",
    "- Avgpool\n",
    "- Eval mode model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal Performance Shaders)\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use GPU\n",
    "    print(\"Using GPU (CUDA)\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Use MPS (for macOS with Apple Silicon)\n",
    "    print(\"Using MPS (Metal Performance Shaders)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Use CPU\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "assert(device is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = \"17\"\n",
    "SUBVERSION=\"\"\n",
    "METADATA_PATH = \"./metadata\"\n",
    "AUXSCALER_PATH = f\"{METADATA_PATH}/auxScaler{VERSION}.pkl\"\n",
    "LABELSCALER_PATH = f\"{METADATA_PATH}/labelScaler{VERSION}.pkl\"\n",
    "MODEL_PATH = f\"{METADATA_PATH}/model{VERSION}{SUBVERSION}.pth\"\n",
    "FIG_PATH = f\"{METADATA_PATH}/fig{VERSION}{SUBVERSION}.png\"\n",
    "SUBMISSION_PATH = f\"{METADATA_PATH}/submission{VERSION}{SUBVERSION}.csv\"\n",
    "TRAIN_AUX_PATH = \"./data-3/train.csv\"\n",
    "TRAIN_IMAGE_PATH = \"./data-3/train_images\"\n",
    "TEST_AUX_PATH = \"./data-3/test.csv\"\n",
    "TEST_IMAGE_PATH = \"./data-3/test_images\"\n",
    "\n",
    "TRAIN_DATA_RATIO = 0.8\n",
    "\n",
    "# print(*zip(range(500),df.columns),sep=\"\\n\")\n",
    "AUX_START = 1\n",
    "# 1-6 for WORLDCLIM\n",
    "# 7-67 for SOIL\n",
    "# 68-127 for MODIS \n",
    "# 128-163 for VOD\n",
    "AUX_END = 164 #7\n",
    "AUX_CATEGORIES = AUX_END - AUX_START \n",
    "\n",
    "# Labels \n",
    "# (164, 'X4_mean')\n",
    "# (165, 'X11_mean')\n",
    "# (166, 'X18_mean')\n",
    "# (167, 'X26_mean')\n",
    "# (168, 'X50_mean')\n",
    "# (169, 'X3112_mean')\n",
    "\n",
    "LABLES_START = 164\n",
    "LABELS_END = 170\n",
    "LABELS_CATEGORIES = LABELS_END - LABLES_START\n",
    "\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxScaler = MinMaxScaler((0,1))\n",
    "labelScaler = MinMaxScaler((0,1))\n",
    "\n",
    "def addGaussianNoise(mean=0.0, std=0.1):\n",
    "    return lambda tensor: tensor + torch.normal(mean, std, size=tensor.size())\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomVerticalFlip(),\n",
    "    # transforms.RandomAffine(\n",
    "    #     degrees=30,                   # Rotation\n",
    "    #     translate=(0.1, 0.1),         # Translation\n",
    "    #     scale=(0.8, 1.2),             # Scaling\n",
    "    #     shear=(0, 20)                 # Shearing\n",
    "    # ),\n",
    "    # transforms.RandomResizedCrop(96),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    # addGaussianNoise()\n",
    "])\n",
    "\n",
    "test_image_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "input_transform = transforms.Compose([\n",
    "    addGaussianNoise(0, 0.05)\n",
    "])\n",
    "\n",
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, X_ids, X_aux, Y, input_transform, imgMap, transform):\n",
    "        self.X_ids = torch.tensor(X_ids)\n",
    "        self.X_aux = torch.tensor(X_aux, dtype=torch.float32)\n",
    "        self.Y = torch.tensor(Y, dtype=torch.float32)\n",
    "        self.imgIdMap = imgMap\n",
    "        self.transform = transform\n",
    "        self.input_transform = input_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_id = self.X_ids[idx].item()\n",
    "        x_img = self.imgIdMap[str(x_id)]\n",
    "        x_img = self.transform(x_img)\n",
    "\n",
    "        x_aux = self.X_aux[idx]\n",
    "        x_aux = self.input_transform(x_aux)\n",
    "\n",
    "        y = self.Y[idx]\n",
    "\n",
    "        return x_id, x_img, x_aux, y\n",
    "\n",
    "def findOutliers(df):\n",
    "    zdf = np.abs((df - df.mean())/ df.std())\n",
    "    outlier_criteria = zdf < 3\n",
    "    return outlier_criteria.all(axis=1)\n",
    "\n",
    "def preprocessData(filename, handleOutliers=True):\n",
    "    # Load Data\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    if handleOutliers:\n",
    "        # Remove Outliers\n",
    "        previousCount = df.shape[0]\n",
    "        outlierCheckData = df.iloc[:, AUX_START:LABELS_END] # AUX_END, or LABELS_END?\n",
    "        outliers = findOutliers(outlierCheckData)\n",
    "        df = df[outliers]\n",
    "        currentCount = df.shape[0]\n",
    "        print(f\"Removed {-currentCount + previousCount} Outliers! ({previousCount} to {currentCount})\")\n",
    "\n",
    "    imageIds = df[['id']].values.squeeze()\n",
    "    auxData = df.iloc[:, AUX_START:AUX_END]\n",
    "    labels = df.iloc[:, LABLES_START:LABELS_END]\n",
    "\n",
    "    return imageIds, auxData, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 12522 Outliers! (43363 to 30841)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./metadata/labelScaler17.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imageIds, auxData, labels = preprocessData(TRAIN_AUX_PATH, True)\n",
    "# auxData = auxScaler.fit_transform(auxData)\n",
    "# labels = labelScaler.fit_transform(labels)\n",
    "\n",
    "joblib.dump(auxScaler, AUXSCALER_PATH)\n",
    "joblib.dump(labelScaler, LABELSCALER_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19953566772749096"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "X_train, X_test, y_train, y_test = train_test_split(auxData, labels, test_size=0.2)\n",
    "model = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators=100, max_depth=5, learning_rate=0.1)\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "r2_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "timageIds, tauxData, tlabels = preprocessData(TEST_AUX_PATH, False)\n",
    "tauxData = auxScaler.transform(tauxData)\n",
    "\n",
    "\n",
    "with open(SUBMISSION_PATH, \"w\") as file:\n",
    "    columns=['id', 'X4', 'X11', 'X18', 'X26', 'X50', 'X3112']\n",
    "    csvwriter = csv.writer(file)\n",
    "    csvwriter.writerow(columns)\n",
    "    \n",
    "    predictions = model.predict(tauxData)\n",
    "    predictions = labelScaler.inverse_transform(predictions)\n",
    "\n",
    "    data = []\n",
    "    for i,p  in zip(timageIds, predictions):\n",
    "        data.append([i.item()] + list(p))\n",
    "        # print(data)\n",
    "        # break\n",
    "    csvwriter.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
